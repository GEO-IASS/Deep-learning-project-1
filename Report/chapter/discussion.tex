\chapter{Discussion}
\label{chp:disc}

In the previous chapters the implementation and the results were presented. This chapter will discuss the different aspects of these.

The implemented convolutional neural network only consisted of 2 convolutional layers and to fully connected layers to ensure feasibility when training on available computer. However, a validation accuracy of approximately 75\% was achieved on CIFAR-10 dataset.

Different hyperparameters was evaluated and graphs of the loss, training and test accuracy was used to improve these. Different learning rates were evaluated and compared via the graph of the loss function. By looking at the slope of the graph a good learning rate could be chosen.

When determining L2 regularization it was important that it did not overwhelm the data loss. This happens when a too high penalty is chosen. In this report a L2 penalty of 0.001 gave some overfitting while a penalty of 0.01 gave small overfitting but lowered the validation accuracy. For this network with a learning rate of 0.001, a L2 regularization penalty between 0.001 and 0.01 gave small overfitting and a high validation accuracy. Trying values between the two also gave a better result.

After choosing learning rate and l2 regularization penalty a dropout probability was chosen. A dropout of approximately 0.5 or 0.75 gave the best result. However, dropout is also a regularization technique and choosing new dropout values after already minimizing the overfitting might not be the optimal approach. In general though combining L2 regularization and dropout is a good solution. 

A filter size of 3x3 and 5x5 was also tested as the two most common sizes. A filter size of 5x5 performed better with the implemented network training on the CIFAR-10 dataset.

After tuning the previously mentioned hyperparameters different random values were tried. In general it performed with a validation accuracy of 75\% with a few examples giving a lower accuracy. So no better result was really achieved using this approach. This might be because not enough random hyperparameters was tested and because the chosen random hyperparameters was generally too close to the previous choosen parameters. A better result might also be achieved tuning other hyperparameters than the aforementioned.

Finally the convolutional neural network was expanded upon with more convolutional layer, making it a deeper network. The same hyperparameters were used. Simply making the network deeper actually gave a higher validation accuracy. An even higher accuracy might be achievable if the hyperparameters were optimized for this network. Even though simply making the network deeper resulted in a higher accuracy, there is a limit to how many layers will actually keep improving the accuracy of a network. Using a residual neural network would make this boundary higher. Another aspect of adding layers is the higher computation time, just adding two layers in this project gave a much higher execution time. Further adjusting and the hyperparameters for the deeper network was not feasible on the computers available \citep{resnet}. 