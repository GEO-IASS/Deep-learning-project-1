\chapter{Applied theory}
\label{chp:theory}

\section{Convolutional network layers}


\subsection{Convolutional layer}

\subsection{Pooling layer}

\subsection{Fully-connected layer}


\section{Optimization}
Optimization is closely related to the loss function, which evaluates the quality of any particular set of weights W. When doing optimization one actually tries to find weights so that the loss function is minimized. There are many different types of optimizers, however in this course the focus will be on the chosen Adam optimizer. 

\subsection{Adam optimizer}
Adaptive Moment Estimation or \emph{Adam} is a lot like RMSProp, which is a very effective adaptive learning rate method, however with the addition of momentum. In practice Adam often works slightly better than RMSProp, and according to cs231n it is currently recommended as the default \citep{NN3}. Adam also adds bias-correction, so that when a bias is added to the weights, the optimizer takes this into account. 


\section{Hyperparameters}
Hyperparameters are variables set before running one's neural network. They are inputs that can be tweaked in order to optimize the model. 


\subsection{Learning rate}
The learning rate is how quickly a network replaces old beliefs with new ones. An example could be a child learning about cars. If the first 10 cars the child sees are red, then it will believe cars are red and therefore look for red when needing to identify a car. If the child then begins seeing blue cars, then the learning rate determines how fast the child realizes that the red colour is not the most important characteristic of a car. A high learning rate results in fast realization, meaning the network will quickly change its mind. So while one would want a network that quickly can learn new characteristics, it is noteworthy that the learning rate can also be too high, which would result in fluctuating beliefs, and not make the model very precise. 

\subsection{L2 regularization}



\subsection{Dropout}
Dropout is an effective and simple regularization technique, complimenting other methods, such as the L2 regularization described in the section above. 

Dropout works by only keeping neurons active with a certain probability while training. So as shown in figure \ref{fig:dropout} the neurons are not necessarily all active. On the left side all neurons are active and connected, but on the right side seven of the neurons are deactivated causing less overfitting.  

\myFigure{dropout.PNG}{Dropout principle \citep{NN2}}{fig:dropout}{0.6}





